{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.14","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[{"sourceId":7833000,"sourceType":"datasetVersion","datasetId":4590933}],"dockerImageVersionId":30786,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# FeedForward Neural Network for News Topic Classfication","metadata":{}},{"cell_type":"code","source":"# Import Libraries\n\nimport pandas as pd\nimport numpy as np\nfrom collections import Counter\nimport re\nimport matplotlib.pyplot as plt\nfrom sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\nimport random\nfrom time import localtime, strftime\nfrom scipy.stats import spearmanr,pearsonr\nimport zipfile\nimport gc\n\n# fixing random seed for reproducibility\nrandom.seed(123)\nnp.random.seed(123)","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2024-10-26T14:22:09.990714Z","iopub.execute_input":"2024-10-26T14:22:09.991312Z","iopub.status.idle":"2024-10-26T14:22:10.001185Z","shell.execute_reply.started":"2024-10-26T14:22:09.991264Z","shell.execute_reply":"2024-10-26T14:22:09.998848Z"},"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"markdown","source":"## Transform Raw texts into training and development data\n","metadata":{}},{"cell_type":"code","source":"# Loading input data and naming the 2 columns\n\ntrain_df = pd.read_csv(\"/kaggle/input/data-topic/train.csv\",names=[\"label\",\"text\"])\ndev_df = pd.read_csv(\"/kaggle/input/data-topic/dev.csv\",names=[\"label\",\"text\"])\ntest_df = pd.read_csv(\"/kaggle/input/data-topic/test.csv\",names=[\"label\",\"text\"])\n\n#trainsform the df to list\ntrain_text = list(train_df['text'])\ndev_text = list(dev_df['text'])\ntest_text = list(test_df['text'])\n\ntrain_label = np.array(train_df['label'])\ndev_label = np.array(dev_df['label'])\ntest_label = np.array(test_df['label'])","metadata":{"execution":{"iopub.status.busy":"2024-10-26T14:23:08.312630Z","iopub.execute_input":"2024-10-26T14:23:08.313178Z","iopub.status.idle":"2024-10-26T14:23:08.387725Z","shell.execute_reply.started":"2024-10-26T14:23:08.313130Z","shell.execute_reply":"2024-10-26T14:23:08.386172Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"markdown","source":"# Create input representations\n\n### Unigram extraction from a document\n\nYou first need to implement the `extract_ngrams` function. It takes as input:\n- `x_raw`: a string corresponding to the raw text of a document\n- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n- `stop_words`: a list of stop words\n- `vocab`: a given vocabulary. It should be used to extract specific features.\n\nand returns:\n\n- a list of all extracted features.\n","metadata":{}},{"cell_type":"code","source":"stop_words = ['a','in','on','at','and','or',\n              'to', 'the', 'of', 'an', 'by',\n              'as', 'is', 'was', 'were', 'been', 'be',\n              'are','for', 'this', 'that', 'these', 'those', 'you', 'i', 'if',\n             'it', 'he', 'she', 'we', 'they', 'will', 'have', 'has',\n              'do', 'did', 'can', 'could', 'who', 'which', 'what',\n              'but', 'not', 'there', 'no', 'does', 'not', 'so', 've', 'their',\n             'his', 'her', 'they', 'them', 'from', 'with', 'its']\n\ndef extract_ngrams(x_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b',\n                   stop_words=[], vocab=set()):\n\n    pattern_re = re.compile(token_pattern)\n\n    # Tokenize to get unigrams not in the stop list\n    unigrams = [word for word in pattern_re.findall(str(x_raw).lower(),) if word not in stop_words]\n\n    # Initialize the list for all n-grams\n    ngrams_total = []\n\n    if ngram_range[0] == 1:\n        ngrams_total = unigrams\n\n    # Generate n-grams from unigrams\n    xtracted_features = []\n\n    for n in range(ngram_range[0], ngram_range[1] + 1):\n        if n == 1: continue\n\n        args = [unigrams] + [unigrams[i:] for i in range(1, n)]\n\n        xngram = list(zip(*args))\n        xtracted_features.append(xngram)\n\n    for i in xtracted_features:\n        for j in i:\n            ngrams_total.append(j)\n\n    if len(vocab) > 0:\n        ngrams_total = [word for word in ngrams_total if word in vocab]\n\n    return ngrams_total","metadata":{"execution":{"iopub.status.busy":"2024-10-26T14:23:10.334920Z","iopub.execute_input":"2024-10-26T14:23:10.335429Z","iopub.status.idle":"2024-10-26T14:23:10.352045Z","shell.execute_reply.started":"2024-10-26T14:23:10.335381Z","shell.execute_reply":"2024-10-26T14:23:10.350464Z"},"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"### Create a vocabulary of n-grams\n\nThen the `get_vocab` function will be used to (1) create a vocabulary of ngrams; (2) count the document frequencies of ngrams; (3) their raw frequency. It takes as input:\n- `X_raw`: a list of strings each corresponding to the raw text of a document\n- `ngram_range`: a tuple of two integers denoting the type of ngrams you want to extract, e.g. (1,2) denotes extracting unigrams and bigrams.\n- `token_pattern`: a string to be used within a regular expression to extract all tokens. Note that data is already tokenised so you could opt for a simple white space tokenisation.\n- `stop_words`: a list of stop words\n- `min_df`: keep ngrams with a minimum document frequency.\n- `keep_topN`: keep top-N more frequent ngrams.\n\nand returns:\n\n- `vocab`: a set of the n-grams that will be used as features.\n- `df`: a Counter (or dict) that contains ngrams as keys and their corresponding document frequency as values.\n- `ngram_counts`: counts of each ngram in vocab\n","metadata":{}},{"cell_type":"code","source":"def get_vocab(X_raw, ngram_range=(1,3), token_pattern=r'\\b[A-Za-z][A-Za-z]+\\b', min_df=3, keep_topN=3000, stop_words=[]):\n\n    doc_frequency = Counter()\n    ngram_frequency = Counter()\n    vocabulary = set()\n\n    # Loop through each text to calculate frequencies\n    for text in X_raw:\n        ngrams = extract_ngrams(text, ngram_range, token_pattern, stop_words)\n        doc_frequency.update(list(set(ngrams)))\n        ngram_frequency.update(ngrams)\n\n    # Create vocabulary based on minimum document frequency\n    vocabulary = set([i for i in doc_frequency if doc_frequency[i] >= min_df])\n\n    # Optionally keep top N ngrams\n    if keep_topN > 0:\n        vocabulary = set([i[0] for i in ngram_frequency.most_common(keep_topN) if i[0] in vocabulary])\n\n    return vocabulary, doc_frequency, ngram_frequency","metadata":{"execution":{"iopub.status.busy":"2024-10-26T14:23:12.009809Z","iopub.execute_input":"2024-10-26T14:23:12.010892Z","iopub.status.idle":"2024-10-26T14:23:12.024557Z","shell.execute_reply.started":"2024-10-26T14:23:12.010799Z","shell.execute_reply":"2024-10-26T14:23:12.022563Z"},"trusted":true},"execution_count":12,"outputs":[]},{"cell_type":"code","source":"vocab,df,ngram_counts = get_vocab(train_text, ngram_range=(1,1), min_df = 4, keep_topN=3000, stop_words=stop_words)\nvocab_dev, df_dev, ngram_counts_dev = get_vocab(dev_text, ngram_range=(1,1), min_df = 4, keep_topN=3000, stop_words=stop_words)\nvocab_test, df_test, ngram_counts_test = get_vocab(test_text, ngram_range=(1,1), min_df = 4, keep_topN=3000, stop_words=stop_words)\n\nprint(\"Vocab: \\n\", list(vocab)[:50])\nprint(\"\\n Raw frequencies of n-grams: \\n\", df.most_common()[:10])\nprint(\"\\n Counts of each ngram in vocab \\n\", ngram_counts.most_common()[:10])","metadata":{"execution":{"iopub.status.busy":"2024-10-26T14:23:12.818111Z","iopub.execute_input":"2024-10-26T14:23:12.818586Z","iopub.status.idle":"2024-10-26T14:23:13.076141Z","shell.execute_reply.started":"2024-10-26T14:23:12.818536Z","shell.execute_reply":"2024-10-26T14:23:13.074784Z"},"trusted":true},"execution_count":13,"outputs":[{"name":"stdout","text":"Vocab: \n ['quest', 'supplies', 'buried', 'minutes', 'treat', 'braves', 'pending', 'band', 'expansion', 'young', 'credit', 'downer', 'hearing', 'ask', 'nothing', 'troublesome', 'side', 'great', 'income', 'make', 'unique', 'blue', 'depth', 'concern', 'milwaukee', 'number', 'ties', 'reliever', 'federal', 'caused', 'look', 'assistant', 'issued', 'carrier', 'sees', 'tech', 'try', 'others', 'euro', 'brian', 'sports', 'slow', 'vice', 'halliburton', 'promised', 'panthers', 'worst', 'hoped', 'response', 'reported']\n\n Raw frequencies of n-grams: \n [('reuters', 631), ('said', 432), ('tuesday', 413), ('wednesday', 344), ('new', 325), ('after', 295), ('ap', 275), ('athens', 245), ('monday', 221), ('first', 210)]\n\n Counts of each ngram in vocab \n [('reuters', 694), ('said', 440), ('tuesday', 415), ('new', 365), ('wednesday', 346), ('after', 304), ('athens', 293), ('ap', 276), ('monday', 221), ('first', 219)]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Create vocabulary id -> word and word -> vocabulary id dictionaries for reference:","metadata":{}},{"cell_type":"code","source":"def transformation(vocab):\n    # Create word to vocabulary ID mapping\n    id_to_word = dict(enumerate(vocab))\n    # Create vocabulary ID to word mapping\n    word_to_id = {id: word for word, id in id_to_word.items()}\n\n    return id_to_word, word_to_id\n\nid2word_train, word2id_train = transformation(vocab)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T14:23:28.357790Z","iopub.execute_input":"2024-10-26T14:23:28.358998Z","iopub.status.idle":"2024-10-26T14:23:28.367481Z","shell.execute_reply.started":"2024-10-26T14:23:28.358907Z","shell.execute_reply":"2024-10-26T14:23:28.365830Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"### Convert the list of unigrams  into a list of vocabulary indices\n\nStoring actual one-hot vectors into memory for all words in the entire data set is prohibitive. Instead, we will store word indices in the vocabulary and look-up the weight matrix. This is equivalent of doing a dot product between an one-hot vector and the weight matrix.\n\nFirst, represent documents in train, dev and test sets as lists of words in the vocabulary:","metadata":{}},{"cell_type":"code","source":"def uni_indices(vocabulary, word2id):\n\n    # Create an empty list to store the indices for each word in the vocabulary\n    indices_list = []\n    unique_words = list(vocabulary)\n\n    # Loop through each word in the vocabulary\n    for word_id in range(len(unique_words)):\n        list_vocab = []\n        # Check if each word has a corresponding ID in the mapping and collect the IDs\n        for word in unique_words[word_id]:\n            if word in word2id:\n                id = word2id[word]\n                list_vocab.append(id)\n        indices_list.append(list_vocab)\n    return unique_words, indices_list\n\n# Apply the function to the train dataset\nfiltered_train_texts = uni_indices(train_text, word2id_train)\n#print(\"Sample filtered train texts:\", filtered_train_texts[:2])","metadata":{"execution":{"iopub.status.busy":"2024-10-26T14:24:30.915852Z","iopub.execute_input":"2024-10-26T14:24:30.916568Z","iopub.status.idle":"2024-10-26T14:24:30.967511Z","shell.execute_reply.started":"2024-10-26T14:24:30.916515Z","shell.execute_reply":"2024-10-26T14:24:30.965886Z"},"trusted":true},"execution_count":16,"outputs":[]},{"cell_type":"code","source":"# Then convert them into lists of indices in the vocabulary:\n\n# prcessing all document make vocab\nvocab_train, vocab_dev, vocab_test = [],[],[]\nfor text in train_text:\n    vocab_train.append(extract_ngrams(text, ngram_range=(1,1),stop_words=stop_words))\nfor text in dev_text:\n    vocab_dev.append(extract_ngrams(text, ngram_range=(1,1),stop_words=stop_words))\nfor text in test_text:\n    vocab_test.append(extract_ngrams(text, ngram_range=(1,1),stop_words=stop_words))\n\ntrain_unigram,train_indices = uni_indices(vocab_train,word2id_train)\ndev_unigram,dev_indices = uni_indices(vocab_dev,word2id_train)\ntest_unigram,test_indices = uni_indices(vocab_test,word2id_train)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T14:24:54.611353Z","iopub.execute_input":"2024-10-26T14:24:54.611866Z","iopub.status.idle":"2024-10-26T14:24:54.949199Z","shell.execute_reply.started":"2024-10-26T14:24:54.611813Z","shell.execute_reply":"2024-10-26T14:24:54.947524Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"def preprocess_and_pad(data, labels):\n\n    final_label = []\n    final_data = []\n\n    # Filter out empty documents and corresponding labels\n    filtered_data = [d for d in data if d]\n    filtered_labels = [labels[i] for i, d in enumerate(data) if d]\n\n    # Calculate maximum length of the documents\n    max_length = max(len(doc) for doc in filtered_data)\n    print(\"Max length of each sentence: \", max_length)\n\n    # Pad shorter documents with zeros\n    padded_data = [doc + [0] * (max_length - len(doc)) for doc in filtered_data]\n\n    # Ensure the lengths of labels and data are compatible\n    for i in range(len(padded_data)):\n        if len(padded_data[i])==0:\n            continue\n        else:\n            final_data.append(padded_data[i])\n            final_label.append(filtered_labels[i])\n\n    return final_data, np.array(final_label)\n\n\nX_tr,train_label = preprocess_and_pad(train_indices,train_label)\nX_dev,dev_label = preprocess_and_pad(dev_indices,dev_label)\nX_te,test_label = preprocess_and_pad(test_indices,test_label)","metadata":{"execution":{"iopub.status.busy":"2024-10-26T14:25:01.520633Z","iopub.execute_input":"2024-10-26T14:25:01.521337Z","iopub.status.idle":"2024-10-26T14:25:01.545170Z","shell.execute_reply.started":"2024-10-26T14:25:01.521277Z","shell.execute_reply":"2024-10-26T14:25:01.543547Z"},"trusted":true},"execution_count":18,"outputs":[{"name":"stdout","text":"Max length of each sentence:  67\nMax length of each sentence:  35\nMax length of each sentence:  53\n","output_type":"stream"}]},{"cell_type":"code","source":"def list_shape(lst):\n    if isinstance(lst, list):\n        return [len(lst)] + list_shape(lst[0]) if lst else [0]\n    else:\n        return []\n\nprint(\"Shape of the list:\", list_shape(X_tr))\n\nY_tr = train_label\nY_dev = dev_label\nY_te = test_label","metadata":{"execution":{"iopub.status.busy":"2024-10-26T14:25:26.046835Z","iopub.execute_input":"2024-10-26T14:25:26.047359Z","iopub.status.idle":"2024-10-26T14:25:26.056063Z","shell.execute_reply.started":"2024-10-26T14:25:26.047314Z","shell.execute_reply":"2024-10-26T14:25:26.054384Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"Shape of the list: [2400, 67]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Network Architecture\n\nYour network should pass each word index into its corresponding embedding by looking-up on the embedding matrix and then compute the first hidden layer $\\mathbf{h}_1$:\n\n$$\\mathbf{h}_1 = \\frac{1}{|x|}\\sum_i W^e_i, i \\in x$$\n\nwhere $|x|$ is the number of words in the document and $W^e$ is an embedding matrix $|V|\\times d$, $|V|$ is the size of the vocabulary and $d$ the embedding size.\n\nThen $\\mathbf{h}_1$ should be passed through a ReLU activation function:\n\n$$\\mathbf{a}_1 = relu(\\mathbf{h}_1)$$\n\nFinally the hidden layer is passed to the output layer:\n\n\n$$\\mathbf{y} = \\text{softmax}(\\mathbf{a}_1W) $$\nwhere $W$ is a matrix $d \\times |{\\cal Y}|$, $|{\\cal Y}|$ is the number of classes.\n\nDuring training, $\\mathbf{a}_1$ should be multiplied with a dropout mask vector (elementwise) for regularisation before it is passed to the output layer.\n\nYou can extend to a deeper architecture by passing a hidden layer to another one:\n\n$$\\mathbf{h_i} = \\mathbf{a}_{i-1}W_i $$\n\n$$\\mathbf{a_i} = relu(\\mathbf{h_i}) $$\n\n","metadata":{}},{"cell_type":"markdown","source":"# Network Training\n\nFirst we need to define the parameters of our network by initiliasing the weight matrices. For that purpose, you should implement the `network_weights` function that takes as input:\n\n- `vocab_size`: the size of the vocabulary\n- `embedding_dim`: the size of the word embeddings\n- `hidden_dim`: a list of the sizes of any subsequent hidden layers. Empty if there are no hidden layers between the average embedding and the output layer\n- `num_classes`: the number of the classes for the output layer\n\nand returns:\n\n- `W`: a dictionary mapping from layer index (e.g. 0 for the embedding matrix) to the corresponding weight matrix initialised with small random numbers (hint: use numpy.random.uniform with from -0.1 to 0.1)\n\nMake sure that the dimensionality of each weight matrix is compatible with the previous and next weight matrix, otherwise you won't be able to perform forward and backward passes. Consider also using np.float32 precision to save memory.","metadata":{}},{"cell_type":"code","source":"def network_weights(vocab_size=3000, embedding_dim=70,\n                    hidden_dim=[100], num_classes=3, init_val = 0.1):\n\n    weights = {}\n\n    # Initialize weights for the embedding layer\n    weights[0] = np.random.uniform(low=-init_val, high=init_val, size=(vocab_size, embedding_dim)).astype(np.float32)\n\n    # Initialize weights for hidden layers\n    input_dim = embedding_dim\n    for i, dim in enumerate(hidden_dim, 1):\n        weights[i] = np.random.uniform(low=-init_val, high=init_val, size=(input_dim, dim)).astype(np.float32)\n        input_dim = dim\n\n    # Initialize weights for the output layer\n    weights[len(hidden_dim) + 1] = np.random.uniform(low=-init_val, high=init_val, size=(input_dim, num_classes)).astype(np.float32)\n\n    return weights\n\n# Example usage\nW = network_weights(vocab_size=3000, embedding_dim=70, hidden_dim=[100], num_classes=3, init_val=0.1)\nfor k, v in W.items():\n    print(f\"Layer {k} weights shape: {v.shape}\")\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T14:25:55.464634Z","iopub.execute_input":"2024-10-26T14:25:55.465145Z","iopub.status.idle":"2024-10-26T14:25:55.485690Z","shell.execute_reply.started":"2024-10-26T14:25:55.465098Z","shell.execute_reply":"2024-10-26T14:25:55.484001Z"},"trusted":true},"execution_count":20,"outputs":[{"name":"stdout","text":"Layer 0 weights shape: (3000, 70)\nLayer 1 weights shape: (70, 100)\nLayer 2 weights shape: (100, 3)\n","output_type":"stream"}]},{"cell_type":"code","source":"def softmax(z):\n    e_z = np.exp(z-np.max(z))\n    val = e_z / np.sum(e_z)\n    return val\n\ndef categorical_loss(y, y_preds):\n    l = -np.log(y_preds[y])\n    return l\n\ndef relu(z):\n    z = z.copy()\n    a = np.maximum(0, z)\n    return a\n\ndef relu_derivative(z):\n    dz = z.copy()\n    dz[dz <= 0] = 0\n    dz[dz > 0] = 1\n    return dz\n\ndef dropout_mask(size, dropout_rate):\n    # Initialize a vector of 1\n    dropout_vec = np.ones(size)\n\n    dropout_vec[:int(size*dropout_rate)] = 0.0\n    np.random.shuffle(dropout_vec)\n    return dropout_vec\n\nprint(dropout_mask(10, 0.2))\nprint(dropout_mask(10, 0.2))","metadata":{"execution":{"iopub.status.busy":"2024-10-26T14:27:38.911862Z","iopub.execute_input":"2024-10-26T14:27:38.913452Z","iopub.status.idle":"2024-10-26T14:27:38.929935Z","shell.execute_reply.started":"2024-10-26T14:27:38.913368Z","shell.execute_reply":"2024-10-26T14:27:38.928029Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stdout","text":"[1. 1. 1. 1. 0. 1. 0. 1. 1. 1.]\n[1. 1. 1. 0. 1. 1. 0. 1. 1. 1.]\n","output_type":"stream"}]},{"cell_type":"markdown","source":"# Forward Pass\n\nNow you need to implement the `forward_pass` function that passes the input x through the network up to the output layer for computing the probability for each class using the weight matrices in `W`. The ReLU activation function should be applied on each hidden layer.\n\n- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n- `dropout_rate`: the dropout rate that is used to generate a random dropout mask vector applied after each hidden layer for regularisation.\n\nand returns:\n\n- `out_vals`: a dictionary of output values from each layer: h (the vector before the activation function), a (the resulting vector after passing h from the activation function), its dropout mask vector; and the prediction vector (probability for each class) from the output layer.","metadata":{}},{"cell_type":"code","source":"def forward_pass(x, W, dropout_rate=0.2):\n\n    # Initialize lists\n    processed_values = {}\n    hidden_states = []\n    activated_states = []\n    dropout_masks = []\n    num_layers = len(W) - 1\n\n    # Processing input\n    weighted_input = []\n    input_length = len(x)\n\n    # Assign initial layer\n    for i in x:\n        weighted_input.append(W[0][i])\n\n    hidden_sum = np.sum(weighted_input, axis=0)\n    hidden_sum = hidden_sum / input_length\n\n    # Applying Relu and dropout mask\n    activated_sum = relu(hidden_sum)\n    dropout = dropout_mask(len(activated_sum), dropout_rate)\n    processed_output = activated_sum * dropout\n\n    hidden_states.append(hidden_sum)\n    activated_states.append(activated_sum)\n    dropout_masks.append(dropout)\n\n    # Forward pass through layers\n    for i in range(1, num_layers):\n        hidden_output = np.dot(processed_output, W[i])\n\n        activated_output = relu(hidden_output)\n        dropout = dropout_mask(len(activated_output), dropout_rate)\n        processed_output = activated_output * dropout\n        hidden_states.append(hidden_output)\n        activated_states.append(activated_output)\n        dropout_masks.append(dropout)\n\n    # Computing final prediction\n    prediction = softmax(np.dot(processed_output, W[num_layers]))\n\n    # Storing processed values in dictionary\n    processed_values['h'] = hidden_states\n    processed_values['a'] = activated_states\n    processed_values['dropout_vecs'] = dropout_masks\n    processed_values['y'] = prediction\n\n    return processed_values","metadata":{"execution":{"iopub.status.busy":"2024-10-26T14:28:03.019996Z","iopub.execute_input":"2024-10-26T14:28:03.020467Z","iopub.status.idle":"2024-10-26T14:28:03.033841Z","shell.execute_reply.started":"2024-10-26T14:28:03.020422Z","shell.execute_reply":"2024-10-26T14:28:03.032549Z"},"trusted":true},"execution_count":23,"outputs":[]},{"cell_type":"markdown","source":"# Back Propagation\n\nThe `backward_pass` function computes the gradients and updates the weights for each matrix in the network from the output to the input. It takes as input\n\n- `x`: a list of vocabulary indices each corresponding to a word in the document (input)\n- `y`: the true label\n- `W`: a list of weight matrices connecting each part of the network, e.g. for a network with a hidden and an output layer: W[0] is the weight matrix that connects the input to the first hidden layer, W[1] is the weight matrix that connects the hidden layer to the output layer.\n- `out_vals`: a dictionary of output values from a forward pass.\n- `learning_rate`: the learning rate for updating the weights.\n- `freeze_emb`: boolean value indicating whether the embedding weights will be updated.\n\nand returns:\n\n- `W`: the updated weights of the network.\n\nHint: the gradients on the output layer are similar to the multiclass logistic regression.","metadata":{}},{"cell_type":"code","source":"def backward_pass(x, y, W, out_vals, lr=0.001, freeze_emb=False):\n    W_length = len(W)-1\n    W0_shape = W[W_length].shape[0]\n    W1_shape = W[W_length].shape[1]\n\n    # y is the label array\n    y_layer = np.zeros(W[W_length].shape[1])\n    y_layer[y - 1] = 1\n    # Compute the gradient on output layer\n    delta_L = out_vals['y'] - y_layer\n    output_val = out_vals['a'][-1] * out_vals['dropout_vecs'][-1]\n    output_value = output_val.reshape(W0_shape,1)\n    gradient_value = np.dot(output_value,delta_L.reshape(1, W1_shape))\n\n     # Update the temp for layer\n    Wk = np.dot(W[W_length],delta_L).reshape(1,W0_shape)\n    g = out_vals['dropout_vecs'][W_length-1]\n    temp = Wk * g\n    # Update the W\n    W[W_length] = W[W_length] - lr*gradient_value\n\n    for i in range(1, W_length):\n\n        # f'(z) update with activation der\n        der_v = relu_derivative(out_vals['h'][W_length-i]).reshape(1,W[W_length+1-i].shape[0])\n\n        # Compute the gradient on output layer\n        temp = temp * der_v\n        output_v1 = out_vals['a'][W_length-1-i]*out_vals['dropout_vecs'][W_length-1-i]\n        output_value = output_v1.reshape(W[W_length-i].shape[0],1)\n        gradient_value = np.dot(output_value,temp)\n        temp_Wk = np.dot(W[W_length-i],temp.T).reshape(1,W[W_length-i].shape[0])\n        temp_g  = out_vals['dropout_vecs'][W_length-1-i]\n        temp = temp_Wk*temp_g\n\n        # Calculate the new W\n        W[W_length-i] = W[W_length-i] - lr*gradient_value\n\n\n    # Update the W0 if freeze_emb==false\n    if freeze_emb == False:\n        x_array = np.zeros([W[0].shape[0],1])\n        x_array[x] = 1.0\n        lv_1 = relu_derivative(out_vals['h'][0]).reshape(1,W[0].shape[1])\n        temp = temp*lv_1\n        w_gradient = np.dot(x_array,temp)\n        W[0] = W[0] - lr * w_gradient # w[0] not freezze\n\n    return W","metadata":{"execution":{"iopub.status.busy":"2024-10-26T14:28:35.720227Z","iopub.execute_input":"2024-10-26T14:28:35.721368Z","iopub.status.idle":"2024-10-26T14:28:35.743187Z","shell.execute_reply.started":"2024-10-26T14:28:35.721308Z","shell.execute_reply":"2024-10-26T14:28:35.741431Z"},"trusted":true},"execution_count":24,"outputs":[]},{"cell_type":"markdown","source":"# Stochastic Gradient Descent\n\nFinally you need to modify SGD to support back-propagation by using the `forward_pass` and `backward_pass` functions.\n\nThe `SGD` function takes as input:\n\n- `X_tr`: array of training data (vectors)\n- `Y_tr`: labels of `X_tr`\n- `W`: the weights of the network (dictionary)\n- `X_dev`: array of development (i.e. validation) data (vectors)\n- `Y_dev`: labels of `X_dev`\n- `lr`: learning rate\n- `dropout`: regularisation strength\n- `epochs`: number of full passes over the training data\n- `tolerance`: stop training if the difference between the current and previous validation loss is smaller than a threshold\n- `freeze_emb`: boolean value indicating whether the embedding weights will be updated (to be used by the backward pass function).\n- `print_progress`: flag for printing the training progress (train/validation loss)\n\n\nand returns:\n\n- `weights`: the weights learned\n- `training_loss_history`: an array with the average losses of the whole training set after each epoch\n- `validation_loss_history`: an array with the average losses of the whole development set after each epoch","metadata":{}},{"cell_type":"code","source":"def SGD(X_tr, Y_tr, W, X_dev=[], Y_dev=[], lr=0.001, dropout=0.2, epochs=5, tolerance=0.001, freeze_emb=False, print_progress=True):\n    # History of losses for plotting or analysis\n    train_losses = []\n    valid_losses = []\n\n    # Helper function to calculate the mean loss using categorical cross-entropy\n    def calculate_loss(data_X, data_Y, weights, dropout_rate):\n        individual_losses = []\n        for x, y in zip(data_X, data_Y):\n            outputs = forward_pass(x, weights, dropout_rate)\n            prediction = outputs['y']\n            print(prediction)\n\n            # Calculate loss and collect it for averaging\n            current_loss = categorical_loss(y - 1, prediction)\n            individual_losses.append(current_loss)\n        return np.mean(individual_losses)\n\n    # Main loop for each epoch\n    for epoch in range(epochs):\n        # Random shuffle of indices for training data\n        shuffle_indices = np.random.permutation(len(X_tr))\n        X_tr = [X_tr[i] for i in shuffle_indices]\n        Y_tr = [Y_tr[i] for i in shuffle_indices]\n\n        # Loop over each data point and update weights\n        for x, y in zip(X_tr, Y_tr):\n            output_values = forward_pass(x, W, dropout)\n            # Update weights based on gradients\n            W = backward_pass(x, y, W, output_values, lr, freeze_emb)\n\n        # Record the training and validation losses\n        loss_train = calculate_loss(X_tr, Y_tr, W, dropout)\n        train_losses.append(loss_train)\n\n        loss_dev = calculate_loss(X_dev, Y_dev, W, dropout)\n        valid_losses.append(loss_dev)\n\n        print(f\"Epoch {epoch + 1}/{epochs}, Training Loss: {loss_train:.4f}\")\n        print(f\"Epoch {epoch + 1}/{epochs}, Validation Loss: {loss_dev:.4f}\")\n\n    return W, train_losses, valid_losses\n","metadata":{"execution":{"iopub.status.busy":"2024-10-26T14:29:00.419405Z","iopub.execute_input":"2024-10-26T14:29:00.420142Z","iopub.status.idle":"2024-10-26T14:29:00.436428Z","shell.execute_reply.started":"2024-10-26T14:29:00.420083Z","shell.execute_reply":"2024-10-26T14:29:00.434626Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"# Neural Network\n\nTrain and Evaluate by using the network_weights function followed by SGD with Back Propagation.","metadata":{}},{"cell_type":"code","source":"W = network_weights(vocab_size=len(vocab),\n                    embedding_dim = 70,\n                    hidden_dim=[],\n                    num_classes=3)\n\nW, loss_tr, dev_loss = SGD(X_tr, Y_tr,\n                            W,\n                            X_dev=X_dev,\n                            Y_dev=Y_dev,\n                            lr=0.015,\n                            dropout=0.2,\n                            freeze_emb=False,\n                            tolerance=0.01,\n                            epochs=20)","metadata":{"_kg_hide-output":false,"_kg_hide-input":true,"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"# Visualisation of Learning process","metadata":{}},{"cell_type":"code","source":"plt.plot(loss_tr, label='Training loss')\nplt.plot(dev_loss, label='Validation loss')\n\nplt.xlabel('Epochs')\nplt.ylabel('Loss')\nplt.title('Fig 1. Training and Validation Loss History')\nplt.legend()\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-10-26T14:32:27.833304Z","iopub.execute_input":"2024-10-26T14:32:27.833918Z","iopub.status.idle":"2024-10-26T14:32:28.239634Z","shell.execute_reply.started":"2024-10-26T14:32:27.833863Z","shell.execute_reply":"2024-10-26T14:32:28.238165Z"},"trusted":true},"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/plain":"<Figure size 640x480 with 1 Axes>","image/png":"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"},"metadata":{}}]},{"cell_type":"markdown","source":"### Figure **1** - Training and Validation Loss History\nThe graph shows a decrease in both training and validation losses as the model adjusts its weights to minimize the error between its predictions and the actual class labels. The slight fluctuations in the validation loss might indicate that the model could start to overfit the training data in later epochs, especially if these fluctuations become more pronounced or if the validation loss starts increasing in subsequent epochs beyond what was captured. By increasing the number of epochs, we can observe only slight improvement in the loss, hence we can continue using smaller epoch cycles.\n\nThe training and validation loss curves follow a similar curve trend which means that the lessons it's learning are broadly applicable, not just to the specific examples it's seen but also for unseen data. It is also successfully capturing underlying trends that are true more generally.\n\n","metadata":{}},{"cell_type":"markdown","source":"# Evaluation\n\nAccuracy, Precision, Recall and F1-score","metadata":{}},{"cell_type":"code","source":"preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.0)['y']) + 1\n            for x,y in zip(X_te,Y_te)]\nprint('Accuracy:', accuracy_score(Y_te,preds_te))\nprint('Precision:', precision_score(Y_te,preds_te,average='macro'))\nprint('Recall:', recall_score(Y_te,preds_te,average='macro'))\nprint('F1-Score:', f1_score(Y_te,preds_te,average='macro'))","metadata":{"execution":{"iopub.status.busy":"2024-10-26T14:33:24.053561Z","iopub.execute_input":"2024-10-26T14:33:24.054162Z","iopub.status.idle":"2024-10-26T14:33:24.208557Z","shell.execute_reply.started":"2024-10-26T14:33:24.054111Z","shell.execute_reply":"2024-10-26T14:33:24.207057Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stdout","text":"Accuracy: 0.8698553948832035\nPrecision: 0.8720700984098354\nRecall: 0.8697398736529172\nF1-Score: 0.8679274435865844\n","output_type":"stream"}]},{"cell_type":"markdown","source":"### Discuss how did you choose model hyperparameters ?\n\nThe hyperparameters embedding size, learning rate and dropout rate helps us achieve a better result to finetune our model. We uses grid search to get the best parameter that fits our data. By adjusting hyperparameters, you can find a balance between underfitting and overfitting. For example, increasing model complexity might improve training accuracy but can harm generalization to unseen data.\n\n1. Learning rate - The step size should not be too high as it risks overshooting the minimum loss function. But a lower learning rate costs more computational power and takes too long to train. After experimenting with multiple values we find that the learning rate of 0.015 works best for the given data.\n\n2. Embedding size - We experiment over the values 50, 70, 100 sizes. A larger embedding size allows each word to be represented with more features at the risk of overfitting. With 100 size, we find that the model shows a steep decrease in training loss, but struggles to perform well for the validation data. Hence we choose 70 embedding size.  \n\n3. Dropout Rate - Dropout is a technique to drop randomly selected neurons at any time during training. They are temporarily removed in the forward transfer, and any heavy updates will not therefore result in transferring neurons. Out of the dropout rates [0.2, 0.3, 0.4] we find that 0.2 gives the optimal results. during training. They are forever \"right to drop out\". This means that their contributions to downstream neurons are temporarily removed in the forward transfer, and any heavy updates will not therefore result in transferring neurons.","metadata":{}},{"cell_type":"code","source":"learning_rate_list = [0.005, 0.01, 0.05]\nembedding_size_list = [50, 70]\ndrop_out_rate_list = [0.2, 0.4]\ntest_score = []\nall_list = []\nhyper_list = []\n\nresult = []\n\nfor dim in embedding_size_list:\n    for lr in learning_rate_list:\n        for drop in drop_out_rate_list:\n            W = network_weights(vocab_size=len(vocab),\n                    embedding_dim=dim,\n                    hidden_dim=[],\n                    num_classes=3,\n                    init_val = 0.1)\n\n           # We replace the dim, drop and lr for each iter\n            W, loss_tr, dev_loss = SGD(X_tr,Y_tr,W,X_dev=X_dev, Y_dev=Y_dev,lr=lr, dropout = drop,freeze_emb=False,tolerance=0.01,epochs=10,print_progress=True)\n\n            # Results and Scores\n            preds_te = [np.argmax(forward_pass(x, W, dropout_rate=0.2)['y'])+1 for x,y in zip(X_te,test_label)]\n            accuracy = accuracy_score(test_label,preds_te)\n            precision = precision_score(Y_te,preds_te,average='macro')\n            Recall =recall_score(Y_te,preds_te,average='macro')\n            F_Score =f1_score(Y_te,preds_te,average='macro')\n            score = [accuracy, precision, Recall,F_Score]\n\n            current_result = {\n                'embedding_dim': dim,\n                'learning_rate': lr,\n                'dropout_rate': drop,\n                'accuracy': accuracy,\n                'precision': precision,\n                'recall': Recall,\n                'f1_score': F_Score\n            }\n            result.append(current_result)\n\n            test_score.append(accuracy)\n            all_list.append([accuracy, precision, Recall, F_Score])\n            hyper = [dim, lr, drop]\n            hyper_list.append(hyper)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_result = max(test_score)\nbest_id = test_score.index(best_result)\nbest_hyper = hyper_list[best_id]\nbest_S = all_list[best_id]\nprint(\"The best parameter and the F1-score on test set\")\nprint(\"emb_dim:\", best_hyper[0], \" lr:\", best_hyper[1],\" drop_rate:\" ,best_hyper[2] ,\" Accuracy:\", best_result, \"A/P/R/F\",best_S)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}